{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import load\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df_raw = pd.read_csv('data/train_rosters.csv')\n",
    "eval_df_raw = pd.read_csv('data/eval_rosters.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df = train_df_raw.copy()\n",
    "eval_df = eval_df_raw.copy()\n",
    "\n",
    "train_df = train_df.fillna(-1)\n",
    "eval_df = eval_df.fillna(-1)\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pop_features(df, features):\n",
    "    for feat in features:\n",
    "        df.pop(feat)\n",
    "\n",
    "remove_features = ['Unnamed: 0', 'index',\n",
    "                   \"med_num_skills_of_each_worker\", \"avg_num_skills_of_each_worker\",\n",
    "                   \"chi_num_skills_of_each_worker\",\n",
    "                   \"diff_avg_skill_penalty_and_worker_penalty\",\n",
    "                   \"local_skill_demand\",\n",
    "                   \"deviations_skill_staffing_from_mean\",\n",
    "                   \"num_days\", 'num_workers', \"staff_req_sparcity\", \"skill_scarcity\"\n",
    "                   ]\n",
    "\n",
    "\n",
    "\n",
    "pop_features(train_df, remove_features)\n",
    "pop_features(eval_df, remove_features)\n",
    "print(train_df.head)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = train_df.pop('target').values\n",
    "y_eval = eval_df.pop('target').values\n",
    "\n",
    "y_train =  np.ravel(y_train, order='C')\n",
    "y_test =  np.ravel(y_eval, order='C')\n",
    "\n",
    "X_train = train_df.values\n",
    "X_test = eval_df.values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def max_prune(y):\n",
    "    size = len(y)\n",
    "    total_required = np.sum(y)\n",
    "    max_prune = size - total_required\n",
    "    return max_prune/size\n",
    "\n",
    "def confusion_matrix_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return false_negative_rate(cm)\n",
    "\n",
    "def fnr_complement_scorer(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return 1 - false_negative_rate(cm)\n",
    "\n",
    "def false_negative_rate(cm):\n",
    "    return round(cm[1][0] / (cm[1][0] + cm[1][1]), 3)\n",
    "\n",
    "def negative_pred_val(cm):\n",
    "    return round(cm[0][0] / (cm[0][0] + cm[1][0]), 3)\n",
    "\n",
    "def prune_percentage(cm):\n",
    "    return round( (cm[0][0] + cm[1][0]) / (np.sum(cm[:])), 3)\n",
    "\n",
    "def print_metrics(clf):\n",
    "    scores = cross_val_score(clf, X_test, y_test, cv=10, scoring='accuracy')\n",
    "    print(\"Acc cv: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    scores = cross_val_score(clf, X_test, y_test, cv=10, scoring='f1_macro')\n",
    "    print(\"F1 cv: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    #print(\"npr = \", negative_pred_val(cm))\n",
    "    print(\"fnr = \", false_negative_rate(cm))\n",
    "    print(\"prune = \", prune_percentage(cm))\n",
    "\n",
    "def threshold_to_metrics(clf, threshold=0.99):\n",
    "    y_pred_p = clf.predict_proba(X_test)\n",
    "\n",
    "    y_pred_0 = np.where(y_pred_p[:,0] <= threshold, 0, y_pred_p[:,0])\n",
    "    y_pred_1 = y_pred_p[:,1]\n",
    "\n",
    "    y_pred = np.where(y_pred_0 > y_pred_1, 0, 1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fnr = false_negative_rate(cm)\n",
    "    prune_pc = prune_percentage(cm)\n",
    "\n",
    "    return {\"q\":threshold, \"prune\":prune_pc, \"fnr\": fnr, \"cm\":cm}\n",
    "\n",
    "def print_metrics_with_threshold(clf, threshold=0.99):\n",
    "    metrics = threshold_to_metrics(clf, threshold)\n",
    "    print(metrics['cm'])\n",
    "    #print(\"npr = \", negative_pred_val(cm))\n",
    "    print(\"fnr = \", metrics['fnr'])\n",
    "    print(\"prune = \", metrics['prune_pc'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(max_iter=1000, tol=1e-3, loss='log', random_state=0, class_weight='balanced')\n",
    "sgd.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_min = 0\n",
    "x_max = 1.0\n",
    "x_step = .1\n",
    "x_min, x_max, x_step\n",
    "\n",
    "def plot_prune(clf, base_clf=sgd, plot_name=None):\n",
    "    q_metrics = {}\n",
    "    base = {}\n",
    "    for q in np.arange(x_min, x_max, x_step):\n",
    "       q_metrics[q] = threshold_to_metrics(clf, q)['prune']\n",
    "       base[q] = threshold_to_metrics(base_clf, q)['prune']\n",
    "\n",
    "    lists = sorted(q_metrics.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    lists = sorted(base.items()) # sorted by key, return a list of tuples\n",
    "    x_b, y_b = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    l1,  = plt.plot(x, y, label='model 1')\n",
    "    l2,  = plt.plot(x_b, y_b, label='model 0')\n",
    "    plt.legend(handles=[l1, l2], loc='upper right')\n",
    "\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.ylabel(\"Prune Percentage\")\n",
    "\n",
    "    if plot_name is not None:\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(f\"images/{plot_name}\")\n",
    "\n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "def plot_max_prune(clf, base_clf=sgd, plot_name=None):\n",
    "    max_prune_pc = max_prune(y_test)\n",
    "    q_metrics = {}\n",
    "    base = {}\n",
    "    for q in np.arange(x_min, x_max, x_step):\n",
    "       q_metrics[q] = threshold_to_metrics(clf, q)['prune'] / max_prune_pc\n",
    "       base[q] = threshold_to_metrics(base_clf, q)['prune'] / max_prune_pc\n",
    "\n",
    "    lists = sorted(q_metrics.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    lists = sorted(base.items()) # sorted by key, return a list of tuples\n",
    "    x_b, y_b = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    l1,  = plt.plot(x, y, label='model 1')\n",
    "    l2,  = plt.plot(x_b, y_b, label='model 0')\n",
    "    plt.legend(handles=[l1, l2], loc='upper right')\n",
    "    if plot_name is not None:\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(f\"images/{plot_name}\")\n",
    "\n",
    "    plt.show()\n",
    "    return plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_fnr(clf, base_clf=sgd, plot_name=None):\n",
    "    q_metrics = {}\n",
    "    base = {}\n",
    "    for q in np.arange(x_min, x_max, x_step):\n",
    "       q_metrics[q] = threshold_to_metrics(clf, q)['fnr']\n",
    "       base[q] = threshold_to_metrics(base_clf, q)['fnr']\n",
    "\n",
    "    lists = sorted(q_metrics.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    lists = sorted(base.items()) # sorted by key, return a list of tuples\n",
    "    x_b, y_b = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    l1,  = plt.plot(x, y, label='model 1')\n",
    "    l2,  = plt.plot(x_b, y_b, label='model 0')\n",
    "    plt.legend(handles=[l1, l2], loc='upper right')\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.ylabel(\"False Negative Rate\")\n",
    "    if plot_name is not None:\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(f\"images/{plot_name}\")\n",
    "    plt.show()\n",
    "    return plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_prune_single(clf, plot_name=None, range=(0, 1, .05)):\n",
    "    q_metrics = {}\n",
    "    for q in np.arange(range[0], range[1], range[2]):\n",
    "       q_metrics[q] = threshold_to_metrics(clf, q)['prune']\n",
    "\n",
    "\n",
    "    lists = sorted(q_metrics.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.ylabel(\"Prune Percentage\")\n",
    "    if plot_name is not None:\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(f\"images/{plot_name}\")\n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "def plot_fnr_against_prune_single(clf, plot_name=None, range=(0, 1, .05)):\n",
    "    q_metrics = {}\n",
    "    for q in np.arange(range[0], range[1], range[2]):\n",
    "        metrics = threshold_to_metrics(clf, q)\n",
    "        q_metrics[metrics['fnr']] = metrics['prune']\n",
    "\n",
    "\n",
    "    lists = sorted(q_metrics.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"False Negative Rate\")\n",
    "    plt.ylabel(\"Prune Percentage\")\n",
    "    if plot_name is not None:\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(f\"images/{plot_name}\")\n",
    "    plt.show()\n",
    "    return plt\n",
    "\n",
    "\n",
    "def plot_fnr_single(clf, plot_name=None, range=(0, 1, .05)):\n",
    "    q_metrics = {}\n",
    "    for q in np.arange(range[0], range[1], range[2]):\n",
    "       q_metrics[q] = threshold_to_metrics(clf, q)['fnr']\n",
    "\n",
    "\n",
    "    lists = sorted(q_metrics.items()) # sorted by key, return a list of tuples\n",
    "    x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.ylabel(\"False Negative Rate\")\n",
    "    if plot_name is not None:\n",
    "        fig = plt.gcf()\n",
    "        fig.savefig(f\"images/{plot_name}\")\n",
    "    plt.show()\n",
    "    return plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_0 = SGDClassifier(max_iter=1000, tol=1e-3, loss='log', random_state=0)\n",
    "clf_0.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_0, plot_name=\"sgd_fnr_raw_vs_balanced.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_0, plot_name=\"sgd_prune_raw_vs_balanced.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_1 = SGDClassifier(max_iter=1000, tol=1e-3, loss='log', penalty='elasticnet', random_state=0, class_weight='balanced')\n",
    "clf_1.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_2 = SGDClassifier(max_iter=1000, tol=1e-3, loss='modified_huber', random_state=0, class_weight='balanced')\n",
    "clf_2.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_3 = SGDClassifier(max_iter=1000, tol=1e-3, loss='modified_huber', penalty=\"elasticnet\", random_state=0, class_weight='balanced')\n",
    "clf_3.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_3, clf_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_3, clf_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr_single(clf_3, \"sgd_tuned_q_fnr.png\")\n",
    "plot_prune_single(clf_3, \"sgd_tuned_q_pruned.png\")\n",
    "plot_fnr_against_prune_single(clf_3, \"sgd_tuned_fnr_against_prune.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_4 = SGDClassifier(max_iter=1000, tol=1e-3, loss='modified_huber', penalty=\"l1\", random_state=0, class_weight='balanced')\n",
    "clf_4.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_4, clf_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_4, clf_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_5 = LogisticRegression(max_iter=1000, tol=1e-3, random_state=0, class_weight='balanced')\n",
    "clf_5.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_5, clf_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_5, clf_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_6 = LogisticRegression(max_iter=1000, tol=1e-3, random_state=0, solver='sag', class_weight='balanced')\n",
    "clf_6.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_fnr(clf_6, clf_3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prune(clf_6, clf_3)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def roc(clf):\n",
    "    y_score = clf.predict_proba(X_test)\n",
    "    fpr, tpr, t = roc_curve(y_test, y_score[:,0], pos_label=0)\n",
    "    roc = auc(fpr, tpr)\n",
    "\n",
    "    return {\"roc\": roc, \"fpr\": fpr, \"tpr\": tpr}\n",
    "\n",
    "roc_1 = roc(clf_1)\n",
    "roc_2 = roc(clf_2)\n",
    "roc_3 = roc(clf_3)\n",
    "roc_4 = roc(clf_4)\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(roc_1['fpr'], roc_1['tpr'], color='green',\n",
    "         lw=lw, label='ROC dtree (area = %0.3f)' % roc_1['roc'])\n",
    "plt.plot(roc_2['fpr'], roc_2['tpr'], color='red',\n",
    "         lw=lw, label='ROC dtree (area = %0.3f)' % roc_2['roc'])\n",
    "plt.plot(roc_3['fpr'], roc_3['tpr'], color='blue',\n",
    "         lw=lw, label='ROC dtree (area = %0.3f)' % roc_3['roc'])\n",
    "plt.plot(roc_4['fpr'], roc_4['tpr'], color='orange',\n",
    "         lw=lw, label='ROC dtree (area = %0.3f)' % roc_4['roc'])\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Negative Rate')\n",
    "plt.ylabel('True Negative Rate')\n",
    "plt.title('ROC Analysis')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('out.png')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "lr = SGDClassifier(max_iter=1000, tol=1e-3, loss='log', random_state=0)\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:1.0-x, 1:x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch = GridSearchCV(estimator= lr,\n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(),\n",
    "                          n_jobs=-1,\n",
    "                          scoring=fnr_complement_scorer,\n",
    "                          verbose=2).fit(X_train, y_train)\n",
    "\n",
    "#Ploting the score for different values of weight\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': gridsearch.cv_results_['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(weigh_data['weight'], weigh_data['score'])\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "lr = SGDClassifier(max_iter=1000, tol=1e-3, loss='log', random_state=0)\n",
    "\n",
    "#Setting the range for class weights\n",
    "weights = np.linspace(0.0,0.99,200)\n",
    "\n",
    "#Creating a dictionary grid for grid search\n",
    "param_grid = {'class_weight': [{0:1.0-x, 1:x} for x in weights]}\n",
    "\n",
    "#Fitting grid search to the train data with 5 folds\n",
    "gridsearch_2 = GridSearchCV(estimator= lr,\n",
    "                          param_grid= param_grid,\n",
    "                          cv=StratifiedKFold(),\n",
    "                          n_jobs=-1,\n",
    "                          scoring=fnr_prune_scorer,\n",
    "                          verbose=2).fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Ploting the score for different values of weight\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(12,8))\n",
    "weigh_data = pd.DataFrame({ 'score': gridsearch_2.cv_results_['mean_test_score'], 'weight': (1- weights)})\n",
    "sns.lineplot(weigh_data['weight'], weigh_data['score'])\n",
    "plt.xlabel('Weight for class 1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xticks([round(i/10,1) for i in range(0,11,1)])\n",
    "plt.title('Scoring for different class weights', fontsize=24)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(weigh_data.tail(30))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}